#!/usr/bin/env python2.7
"""A concurrent wrapper for timing zope.testrunner tests for opengever.core."""
from argparse import ArgumentParser
from datetime import date
from logging import DEBUG
from logging import FileHandler
from logging import Formatter
from logging import getLogger
from logging import INFO
from logging import StreamHandler
from multiprocessing import cpu_count
from multiprocessing import Pool
from os import devnull
from os import environ
from os import killpg
from os import setpgrp
from signal import SIGINT
from signal import SIGKILL
from signal import signal
from subprocess import CalledProcessError
from subprocess import check_output
from sys import stdout
from time import time
import re


def parse_arguments():
    """Parse and return passed in command line arguments."""
    parser = ArgumentParser(description="Run tests in parallel.")
    parser.add_argument(
        "-d", "--debug", action="store_true", help="Enable debug logging"
    )
    parser.add_argument(
        "-l", "--layer", help="Greedy match test layer name.", action="append"
    )
    parser.add_argument(
        "-m", "--module", help="Greedy match module name.", action="append"
    )
    return parser.parse_args()


def setup_termination():
    """Set up process group mass infanticide for the runner.

    * Set the group flag so that subprocesses will be in the same group.
    * Kill the group (including main process) on terminal signal.
    """
    setpgrp()

    def terminate(signum, frame):
        logger.debug("SIGKILL received!")
        logger.debug("%d, %s", signum, frame)
        killpg(0, SIGKILL)

    signal(SIGINT, terminate)


def setup_logging(debug=False):
    """Set up a tee-esque logger for the results output."""
    today = date.today()
    logfile = "{:04}-{:02}-{:02}-moduleperf.log".format(
        today.year, today.month, today.day
    )

    teelogger = getLogger("opengever-time-layers")
    formatter = Formatter("")

    filehandler = FileHandler(logfile, mode="w")
    filehandler.setFormatter(formatter)

    stdouthandler = StreamHandler(stream=stdout)
    stdouthandler.setFormatter(formatter)

    if debug:
        teelogger.setLevel(DEBUG)
    else:
        teelogger.setLevel(INFO)

    teelogger.addHandler(filehandler)
    teelogger.addHandler(stdouthandler)

    return teelogger


def humanize_time(seconds):
    """Humanize a seconds based delta time.

    Only handles time spans up to weeks for simplicity.

    """
    seconds = abs(seconds)
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    days, hours = divmod(hours, 24)
    weeks, days = divmod(days, 7)

    seconds = int(seconds)
    minutes = int(minutes)
    hours = int(hours)
    days = int(days)
    weeks = int(weeks)

    output = []

    if weeks:
        output.append("{:02d} weeks".format(weeks))
    if days:
        output.append("{:02d} days".format(days))
    if hours:
        output.append("{:02d} hours".format(hours))
    if minutes:
        output.append("{:02d} minutes".format(minutes))

    output.append("{:02d} seconds".format(seconds))

    return " ".join(output)


def discover_tests(layers=None, modules=None):
    """Discover all sets of layer-module combinations.

    Results can be limited by passing in a list of layers and/or modules to
    greedy match.

    There is one special case we try to handle for the user:
    ``opengever.task`` also greedy matches ``opengever.tasktemplates``.
    """
    if layers is None:
        layers = ()
        logger.debug("Timing for all layers.")

    if modules is None:
        modules = ()
        logger.debug("Timing for all modules.")

    task_in_modules = any("task" in module for module in modules)
    tasktemplates_in_modules = any("tasktemplates" in module for module in modules)
    if task_in_modules:
        if not tasktemplates_in_modules:
            modules.append("!opengever.tasktemplates")
            logger.debug("Explicitly excluding opengever.tasktemplates.")

    params = ["bin/test"]
    for layer in layers:
        params.append("--layer")
        params.append(layer)
        logger.debug("Going to greedy match a layer with: %s.", layer)
    for module in modules:
        params.append("-m")
        params.append(module)
        logger.debug("Going to greedy match a module with: %s.", module)
    params.append("--list-tests")

    env = environ.copy()
    logger.debug("Discovering tests.")
    start_time = time()
    with open(devnull, "wb") as dn:
        discovery = check_output(params, stderr=dn, env=env, universal_newlines=True)
    logger.debug("Discovered tests in %s.", humanize_time(time() - start_time))

    permutations = {}
    layer = None
    classname = None
    module = None

    for line in discovery.splitlines():
        if line.startswith("Listing"):
            layer = re.search("^Listing (.*) tests:", line).groups()[0]
            permutations[layer] = {}

        # All listed tests are indented with 2 spaces
        if layer and line.startswith("  "):
            if "(" in line:
                classname = re.search(r".*\((.*)\).*", line).groups()[0]
                module = ".".join(classname.split(".")[:2])

            # Count discovered tests per layer-module permutation
            if not permutations[layer].get(module):
                permutations[layer][module] = 0

            permutations[layer][module] += 1

    return permutations


def create_test_runs(layers=None, modules=None):
    """Create per layer per module test runs.

    We need to special case opengever.task as that also greedy matches
    opengever.tasktemplates.
    """
    test_runs = []
    port = 0

    for layer, modules_ in discover_tests(layers=layers, modules=modules).items():
        for module, count in modules_.items():
            test_run = {"module": module, "layer": layer, "count": count, "port": port}
            port += 1
            if "task" in module and "tasktemplates" not in module:
                test_run["module_exclusion"] = "!opengever.tasktemplates"
            test_runs.append(test_run)

    return test_runs


def parse_time(time_string):
    """Parse ``zope.testrunner`` human readable runtimes."""
    minutes = re.search(r"(\d+) minutes", time_string)
    if minutes:
        minutes = int(minutes.groups()[0])
    else:
        minutes = 0

    seconds = re.search(r"(\d+\.\d+) seconds", time_string)
    if seconds:
        seconds = float(seconds.groups()[0])
    else:
        seconds = 0.0

    seconds += minutes * 60

    return seconds


def run_tests(test_run_params):
    """Run and time ``bin/test --layer layer -m module``.

    We need to special case modules as those also can also greedy match some
    of the other modules. This is currently not so for layers.

    We use ``pytimeparse`` to parse the actual reported test run time from
    ``zope.testrunner`` in order to exclude the irrelevant duplicated test
    discovery and layer setup times.
    """
    layer = test_run_params["layer"]
    module = test_run_params["module"]

    params = ["bin/test", "--layer", layer, "-m", module]
    module_exclusion = test_run_params.get("module_exclusion")
    if module_exclusion:
        params.append("-m")
        params.append(module_exclusion)

    env = environ.copy()
    env["ZSERVER_PORT"] = str(55000 + test_run_params["port"])

    try:
        with open(devnull, "wb") as dn:
            output = check_output(params, stderr=dn, env=env, universal_newlines=True)
            failed = False
    except CalledProcessError as e:
        output = e.output
        failed = True

    try:
        for line in output.splitlines():
            if line.startswith("  Ran"):
                count = int(line.split()[1])
                runtime = parse_time(line.split(" in ")[-1].strip("."))
                break
        speed = runtime / count
        return {
            "layer": layer,
            "module": module,
            "count": count,
            "runtime": runtime,
            "speed": speed,
            "failed": failed,
        }
    except BaseException:
        return {"layer": layer, "module": module, "failed": True}


def main(layers=None, modules=None):
    """Discovers and times tests in parallel via multiprocessing.Pool()."""
    test_runs = create_test_runs(layers=layers, modules=modules)
    discovered_layers = set(param.get("layer", "Unknown layer") for param in test_runs)
    for layer in discovered_layers:
        logger.debug("Discovered: %s", layer)
    logger.debug("Discovered %d layers in total.", len(discovered_layers))

    discovered_modules = set(
        param.get("module", "Unknown module") for param in test_runs
    )
    for module in discovered_modules:
        logger.debug("Discovered: %s", module)
    logger.debug("Discovered %d modules in total.", len(discovered_modules))

    logger.debug("Running %d test runs.", len(test_runs))

    # Counter system congestion and hyperthreading, FWIW
    concurrency = max(1, cpu_count() // 2 - 1)
    logger.debug("Timing tests in up to %d processes in parallel.", concurrency)
    pool = Pool(concurrency)

    logger.debug("Timing layers - this can take a while!")
    start_time = time()
    results = sorted(
        pool.imap_unordered(run_tests, test_runs),
        key=lambda result: result.get("runtime", 0.0),
    )

    pool.terminate()
    pool.join()

    wallclock = humanize_time(time() - start_time)
    logger.debug("Done timing layers in %s.", wallclock)

    total_runtime = sum(result.get("runtime", 0.0) for result in results)
    total_count = sum(result.get("count", 0) for result in results)

    layer_width = max(len(layer) for layer in discovered_layers)
    module_width = max(len(module) + 4 for module in discovered_modules)
    count_width = max(len(str(result.get("count", 0))) + 4 for result in results)
    speed_width = max(
        len("{:.3f}".format(result.get("speed", 0))) + 4 for result in results
    )
    runtime_width = max(
        len(humanize_time(result.get("runtime", 0.0))) + 4 for result in results
    )

    header = (
        "{layer:>{layer_width}}"
        "{module:>{module_width}}"
        "{count:>{count_width}}"
        "{speed:>{speed_width}}"
        "{runtime:>{runtime_width}}"
        "{runtime_percentage:>10}"  # 9.2f
        "{count_percentage:>10}"  # 9.2f
        "{relative_weight:>11}".format(  # 10.2f
            layer="layer",
            module="module",
            count="cnt",
            speed="spd",
            runtime="rt",
            runtime_percentage="rt%",
            count_percentage="cnt%",
            relative_weight="wt%",
            layer_width=layer_width,
            module_width=module_width,
            count_width=count_width + 6,  # Suffix " tests"
            speed_width=speed_width + 9,  # Suffix " s / test"
            runtime_width=runtime_width,
        )
    )
    logger.info(header)
    header_width = len(header)
    logger.info("=" * header_width)

    for result in results:
        layer = result.get("layer", "Unknown layer")
        module = result.get("module", "Unknown module")
        count = result.get("count", 0)
        runtime = result.get("runtime", 0.0)
        speed = result.get("speed", 0.0)
        runtime = result.get("runtime", 0)

        runtime_percentage = runtime / total_runtime
        count_percentage = float(count) / float(total_count)
        try:
            relative_weight = runtime_percentage / count_percentage
        except ZeroDivisionError:
            # Something failed and count thus is 0
            relative_weight = 0.0

        runtime = humanize_time(runtime)
        line = (
            "{layer:>{layer_width}}"
            "{module:>{module_width}}"
            "{count:>{count_width}} tests"
            "{speed:>{speed_width}.3f} s / test"
            "{runtime:>{runtime_width}}"
            "{runtime_percentage:9.2f}%"
            "{count_percentage:>9.2f}%"
            "{relative_weight:>10.2f}%".format(
                layer=layer,
                module=module,
                count=count,
                speed=speed,
                runtime=runtime,
                runtime_percentage=runtime_percentage * 100,
                count_percentage=count_percentage * 100,
                relative_weight=relative_weight * 100,
                layer_width=layer_width,
                module_width=module_width,
                count_width=count_width,
                speed_width=speed_width,
                runtime_width=runtime_width,
            )
        )
        logger.info(line)

    total = humanize_time(total_runtime)
    total_runtime_width = len(total)
    wallclock_width = len(wallclock)
    totals_width = max(wallclock_width, total_runtime_width)

    total_line = "Total:     {:>{totals_width}}".format(
        total, totals_width=totals_width
    )
    wallclock_line = "Wallclock: {:>{totals_width}}".format(
        wallclock, totals_width=totals_width
    )
    logger.info("-" * header_width)
    logger.info("Sorted by runtime.")
    logger.info("")
    logger.info(total_line)
    logger.info(wallclock_line)

    failed_runs = [result for result in results if result.get("failed")]
    if failed_runs:
        logger.warn("Test run failures detected - YMMV!")
    for run in failed_runs:
        logger.warn(
            "Failures in: %s - %s",
            run.get("layer", "Unknown layer"),
            run.get("module", "Unknown module"),
        )


if __name__ == "__main__":
    # Having the __main__ guard is necessary for multiprocessing.Pool().
    args = parse_arguments()
    logger = setup_logging(debug=args.debug)
    setup_termination()
    main(layers=args.layer, modules=args.module)
