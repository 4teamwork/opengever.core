#!/usr/bin/env python
"""A concurrent wrapper for timing xmltestrunner tests for buildout.coredev."""
from __future__ import print_function
from argparse import ArgumentParser
from fnmatch import fnmatch
from logging import Formatter
from logging import getLogger
from logging import INFO
from logging import StreamHandler
from logging.handlers import MemoryHandler
from math import ceil
from multiprocessing import cpu_count
from multiprocessing import Pool
from os import access
from os import devnull
from os import environ
from os import killpg
from os import path
from os import pathsep
from os import setpgrp
from os import unlink
from os import walk
from os import X_OK
from signal import SIGINT
from signal import SIGKILL
from signal import signal
from subprocess import CalledProcessError
from subprocess import check_output
from subprocess import STDOUT
from time import time
import re
import sys


def which(program):
    def is_exe(fpath):
        return path.isfile(fpath) and access(fpath, X_OK)

    fpath, fname = path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for dpath in environ["PATH"].split(pathsep):
            exe_file = path.join(dpath, program)
            if is_exe(exe_file):
                return exe_file

    return None


def humanize_time(seconds):
    """Humanize a seconds based delta time.

    Only handles time spans up to weeks for simplicity.
    """
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    days, hours = divmod(hours, 24)
    weeks, days = divmod(days, 7)

    seconds = int(seconds)
    minutes = int(minutes)
    hours = int(hours)
    days = int(days)
    weeks = int(weeks)

    output = []

    if weeks:
        quantifier = 'weeks' if weeks > 1 or weeks == 0 else 'week'
        output.append('{} {}'.format(weeks, quantifier))
    if days:
        quantifier = 'days' if days > 1 or days == 0 else 'day'
        output.append('{} {}'.format(days, quantifier))
    if hours:
        quantifier = 'hours' if hours > 1 or hours == 0 else 'hour'
        output.append('{} {}'.format(hours, quantifier))
    if minutes:
        quantifier = 'minutes' if minutes > 1 or minutes == 0 else 'minute'
        output.append('{} {}'.format(minutes, quantifier))

    quantifier = 'seconds' if seconds > 1 or seconds == 0 else 'second'
    output.append('{} {}'.format(seconds, quantifier))

    return ' '.join(output)


def setup_termination():
    # Set the group flag so that subprocesses will be in the same group.
    setpgrp()

    def terminate(signum, frame):
        # Kill the group (including main process) on terminal signal.
        killpg(0, SIGKILL)

    signal(SIGINT, terminate)


def discover_tests(layers=None, modules=None):
    logger.info('Discovering tests.')
    memory_handler.flush()

    batches = {}

    layer = None
    classname = None

    for line in test_discovery(layers, modules):
        if line.startswith('Listing'):
            layer = re.search('^Listing (.*) tests:', line).groups()[0]
            batches[layer] = {}

        # All listed tests are indented with 2 spaces
        if layer and line.startswith('  '):
            if '(' in line:
                classname = re.search(r'.*\((.*)\).*', line).groups()[0]

            # Count discovered tests per testclass
            if not batches.get(layer).get(classname):
                batches[layer][classname] = 0

            batches[layer][classname] += 1

    return batches


def test_discovery(layers=None, modules=None):
    if not layers:
        layers = ()
    if not modules:
        modules = ()

    cmdline = ['bin/test', '--list-tests']
    for layer in layers:
        cmdline.append('--layer')
        cmdline.append(layer)
    for module in modules:
        cmdline.append('-m')
        cmdline.append(module)

    with open(devnull, 'w') as f:
        output = check_output(cmdline, stderr=f, universal_newlines=True).splitlines()

    return output


def chunks(chunkable, chunksize):
    output = []
    while chunkable:
        output.append([])
        size = 0
        while chunkable and size < chunksize:
            output[-1].append(chunkable.pop(0))
            size = sum(testclass[-1] for testclass in output[-1])
    return output


def split(batch):
    minsize_mapping = {
        'opengever.core.testing.opengever.core:integration': 200,
        'opengever.core.testing.opengever.core:functional': 100,
    }

    maxsplit_mapping = {
        'opengever.core.testing.opengever.core:integration': 4,
        'opengever.core.testing.opengever.core:functional': 2,
    }

    layer = batch.get('layer')

    unsplittable_layer = (
        layer not in minsize_mapping
        or
        layer not in maxsplit_mapping
    )

    if unsplittable_layer or CONCURRENCY < 2:
        batch['count'] = sum(batch.get('testclasses').values())
        del batch['testclasses']
        return (batch, )

    original_count = batch.get('original_count')
    chunk_count = ceil(original_count / float(minsize_mapping.get(layer)))
    chunk_count = min(chunk_count, CONCURRENCY)
    chunk_count = min(chunk_count, maxsplit_mapping.get(layer))
    chunksize = original_count / float(chunk_count)

    splinters = []
    for chunk in chunks(batch.get('testclasses').items(), chunksize):
        splinters.append({'layer': layer, 'testclasses': dict(chunk), 'original_count': original_count})
        splinters[-1]['count'] = sum(splinters[-1].get('testclasses').values())

    return tuple(sorted(
        splinters,
        key=lambda batch: -batch.get('count'),
        ))


def create_test_run_params(layers=None, modules=None, shuffle=False):
    test_run_params = []

    for layer, testclasses in discover_tests(layers, modules).iteritems():
        batch = {}
        batch['layer'] = layer
        batch['testclasses'] = testclasses
        batch['original_count'] = sum(testclasses.values())
        split_batches = split(batch)
        for i, split_batch in enumerate(split_batches):
            split_batch['batchinfo'] = '{}/{}'.format(i + 1, len(split_batches))
            split_batch['shuffle'] = shuffle
            test_run_params.append(split_batch)

    return tuple(sorted(
        test_run_params,
        key=lambda batch: -batch.get('original_count'),
        ))


def remove_bytecode_files(directory_path):
    logger.info('Removing bytecode files from %s', directory_path)

    for filename in find_bytecode_files(directory_path):
        unlink(filename)


def find_bytecode_files(directory_path):
    for root, _, files in walk(directory_path):
        for name in files:
            if fnmatch(name, '*.py[co]'):
                yield path.join(root, name)


def run_tests(test_run_params):
    """Run and time 'bin/test --layer layer -m module [-m module]'."""
    params = ['bin/test']
    params.append('--xml')

    layer = test_run_params.get('layer')
    batchinfo = test_run_params.get('batchinfo')
    testclasses = test_run_params.get('testclasses', ())
    count = test_run_params.get('count')
    shuffle = test_run_params.get('shuffle')

    if shuffle:
        params.append('--shuffle')

    if layer:
        params.append('--layer')
        params.append(layer)

    for testclass in testclasses:
        params.append('-t')
        params.append(testclass)

    printable_params = ' '.join(["'{}'".format(param) if ' ' in param else param for param in params])

    logger.info(
        "START - %s %s - %d %s",
        layer,
        batchinfo,
        count,
        'test' if count == 1 else 'tests',
        )

    # Explicitly flush to trigger IPC over cPickle
    memory_handler.flush()

    env = environ.copy()
    # We cannot access cached sqlite fixtures in parallel yet
    env['GEVER_CACHE_TEST_DB'] = 'false'

    start = time()

    try:
        output = check_output(params, stderr=STDOUT, env=env, universal_newlines=True)
        returncode = 0
    except CalledProcessError as e:
        output = e.output
        returncode = e.returncode

    runtime = time() - start

    result = {
        'layer': layer,
        'returncode': returncode,
        'runtime': runtime,
        }

    done_args = (
        'DONE - %s %s - %d %s in %s',
        layer,
        batchinfo,
        count,
        'test' if count == 1 else 'tests',
        humanize_time(runtime),
        )

    if returncode:
        log_output.error('')
        log_output.error('Command line')
        log_output.error('')
        log_output.error(printable_params)
        log_output.error('')
        log_output.error(output)
        log_output.error('')
        logger.error(*done_args)
    else:
        logger.info(*done_args)

    # Explicitly flush to trigger IPC over cPickle (and to avoid
    # carryover-fragment-via-pickling)
    memory_handler.flush()

    return result


def main(layers=None, modules=None, shuffle=False):
    """Discovers and times tests in parallel via multiprocessing.Pool()."""
    # Remove *.py[co] files to avoid race conditions with parallel workers
    # stepping on each other's toes when trying to clean up stale bytecode.
    #
    # Setting PYTHONDONTWRITEBYTECODE is not enough, because running buildout
    # also already precompiles bytecode for some eggs.
    remove_bytecode_files(OPENGEVER_PATH)
    remove_bytecode_files(SOURCE_PATH)

    start = time()
    test_run_params = create_test_run_params(layers, modules, shuffle)
    logger.info('Discovered tests in %s', humanize_time(time() - start))
    logger.info('Split the tests into %d jobs', len(test_run_params))
    logger.info('Running the jobs in up to %d processes in parallel', CONCURRENCY)
    if args.shuffle:
        logger.info('Shuffling each test batch run order.')

    # We need to explicitly flush here in order to avoid multiprocessing
    # related log output duplications due to picking inputs and globals as the
    # default IPC mechanism
    memory_handler.flush()

    failed_tests = set()
    job_count = len(test_run_params)
    runtime = 0

    start = time()

    pool = Pool(CONCURRENCY)

    for result in pool.imap_unordered(run_tests, test_run_params):
        runtime += result.get('runtime')
        if result.get('returncode', 1):
            failed_tests.add(result.get('layer'))

    pool.close()
    pool.join()

    error_count = len(failed_tests)

    if error_count:
        logger.error('%d / %d jobs failed.', error_count, job_count)
        for layer in failed_tests:
            logger.error('Failures in %s', layer)

    logger.info('Aggregate runtime %s.', humanize_time(runtime))
    logger.info('Wallclock runtime %s.', humanize_time(time() - start))

    if not error_count:
        logger.info('No failed tests.')
        return True

    return False


# Having the __main__ guard is necessary for multiprocessing.Pool().
if __name__ == '__main__':
    # Globals
    environ['PYTHONUNBUFFERED'] = '1'
    environ['PYTHONDONTWRITEBYTECODE'] = '1'

    CONCURRENCY = int(cpu_count())
    BUILDOUT_PATH = path.abspath(path.join(__file__, '..', '..'))
    OPENGEVER_PATH = path.join(BUILDOUT_PATH, 'opengever')
    SOURCE_PATH = path.join(BUILDOUT_PATH, 'src')

    # CLI arguments
    parser = ArgumentParser(
        description='Run tests in parallel.',
        )

    parser.add_argument(
        '--shuffle',
        action='store_true',
        help='Set the testing concurrency level.',
        )

    parser.add_argument(
        '-j',
        '--jobs',
        type=int,
        help='Set the testing concurrency level.',
        )

    parser.add_argument(
        '-l',
        '--layer',
        help='Greedy match test layer name.',
        action='append',
        )

    parser.add_argument(
        '-m',
        '--module',
        help='Greedy match module name.',
        action='append',
        )

    args = parser.parse_args()

    if args.jobs:
        CONCURRENCY = int(args.jobs)

    default_loglevel = INFO

    # Logging
    logger = getLogger('mtest')
    logger.setLevel(default_loglevel)

    # Set up logging to stdout
    stream_handler = StreamHandler()
    stream_handler.setLevel(default_loglevel)
    log_formatter = Formatter(
        ' - '.join((
            '%(asctime)s',
            '%(levelname)s',
            '%(message)s',
            )),
        )
    stream_handler.setFormatter(log_formatter)
    # Buffer log messages so we do not get broken-by-racecondition lines
    memory_handler = MemoryHandler(2, target=stream_handler)
    memory_handler.setLevel(default_loglevel)
    logger.addHandler(memory_handler)

    # Set up a separate logger for writing failure output to stdout. We do this
    # because the logging module handles I/O encoding properly, whereas with
    # 'print' we'd need to do it ourselves. (Think piping the output of
    # bin/mtest somewhere, or shell I/O redirection).
    log_output = getLogger('mtest.output')
    log_output.propagate = False
    stdout_handler = StreamHandler(stream=sys.stdout)
    stdout_handler.setFormatter(Formatter(''))
    log_output.addHandler(stdout_handler)
    log_output.setLevel(INFO)

    setup_termination()

    if main(layers=args.layer, modules=args.module, shuffle=args.shuffle):
        exit(0)

    exit(1)
