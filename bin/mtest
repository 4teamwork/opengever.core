#!/usr/bin/env python
"""Run tests in configurably many subprocesses and preconfigurable buckets."""
from __future__ import print_function
from copy import copy
from fnmatch import fnmatch
from multiprocessing import cpu_count
from multiprocessing import Pool
import os
import subprocess
import sys


def main():
    """Discover and run tests.

    Remove *.py[co] files to avoid race conditions with parallel workers
    stepping on each other's toes when trying to clean up stale bytecode.

    Setting PYTHONDONTWRITEBYTECODE is not enough, because running buildout
    also already precompiles bytecode for some eggs.
    """
    os.environ['PYTHONDONTWRITEBYTECODE'] = '1'

    buildout_path = os.path.abspath(os.path.join(__file__, '..', '..'))
    opengever_path = os.path.join(buildout_path, 'opengever')

    remove_bytecode_files(opengever_path)
    remove_bytecode_files('src')

    print('Discovering modules.')

    # Assuming hyperthreading and non-dedicated use for the default core count
    worker_count = int(
        os.environ.get('MTEST_PROCESSORS', cpu_count() // 2 - 1))

    buckets = discover_and_bucket_modules()
    batches = split_buckets_to_batches(buckets, worker_count)

    print('Running tests with {} workers.'.format(worker_count))

    pool = Pool(worker_count)
    results = pool.map(run_tests, batches)

    if not any([result.get('returncode', None) != 0 for result in results]):
        print('No failed tests.')
        sys.exit(0)

    for result in results:
        # UTF-8 is explicitly expected as the runtime locale!
        output = result.get('output')
        for line in output:
            print(line.decode('UTF-8'), end='')

    # Fail per default.
    sys.exit(1)


def remove_bytecode_files(path):
    """Ensure clean test runs by nuking prcompiled bytecode."""
    print('Removing bytecode files from {}'.format(path))
    for filename in find_bytecode_files(path):
        os.unlink(filename)


def find_bytecode_files(path):
    """Discover bytecode files."""
    for root, _, files in os.walk(path):
        for name in files:
            if fnmatch(name, '*.py[co]'):
                yield os.path.join(root, name)


def run_tests(batch):
    """Run tests for a batch of modules."""
    arguments = ['bin/test']

    module_arguments = tuple([
        ('-m', module, )
        for module in batch.get('modules', None)
        if module
    ])

    # Unpack the module argument tuples flat
    arguments.extend(sum(module_arguments, ()))

    env = copy(os.environ)
    # We need a port provided or we hard fail
    env['ZSERVER_PORT'] = batch.get('port')

    test_run = subprocess.Popen(
        arguments,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        env=env,
        )

    # The pipe buffer is only 4k in size, so we need to constantly fetch it
    output = test_run.communicate()
    returncode = test_run.returncode

    if returncode == 0:
        # Only return output if we have a failed test
        output = None

    result = dict(
        output=output,
        returncode=returncode,
        )

    return result


def discover_and_bucket_modules():
    """Discover modules and manually prebucket them per expected runtime."""
    very_slow = (
        'meeting',
        )

    slow = (
        'base',
        'document',
        'dossier',
        'task',
        )

    normal = tuple([
        name
        for name in os.listdir('opengever')
        if name[0].isalpha()
        if name not in slow
        if name not in very_slow
        ])

    buckets = dict(
        slow=slow,
        very_slow=very_slow,
        normal=normal,
    )

    return buckets


def split_buckets_to_batches(buckets, worker_count):
    normal = buckets.get('normal', [])
    slow = buckets.get('slow', [])
    very_slow = buckets.get('very_slow', [])

    if worker_count < 2:
        # Just run everything sequentially in one job
        batches = sum(buckets, ())

    else:
        # Create as many test batches as there are non-dedicated runners
        free_cores = worker_count - len(very_slow)
        batches = [[] for _ in range(free_cores)]

        # Round robin the known slow modules across the test runners
        for i, module in enumerate(slow + normal):
            batches[i % free_cores].append(module)

        # Append the known very slow modules as their dedicated test runners
        for module in very_slow:
            batches.append([module])

    # Guard against empty batches in case of more workers than modules
    # Get worker ZSERVER_PORT from the jenkins port allocator
    # The jenkins port allocator enumerates starting from 1
    # Default to 55000 + i
    batches = [
        dict(
            modules=batch,
            port=os.environ.get('PORT{}'.format(i + 1), str(55000 + i)),
            )
        for i, batch in enumerate(batches)
        if batch
        ]

    return batches


if __name__ == '__main__':
    main()
